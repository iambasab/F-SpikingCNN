# -*- coding: utf-8 -*-
"""mnist.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1R2eVmyeLRSENhc69iHjyTsdmIr1y2uSP
"""

! pip install --upgrade gitdb
! pip install nengo-dl 
! pip install nengo

import nengo
import nengo_dl

# Loading Data

# Commented out IPython magic to ensure Python compatibility.
import tensorflow as tf
import numpy as np
from tensorflow.examples.tutorials.mnist import input_data
from scipy.ndimage import correlate
# %matplotlib inline 
import matplotlib.pyplot as plt

mnist = input_data.read_data_sets("MNIST_data/", one_hot=True)
print(mnist.train.labels[0])
num_classes = mnist.train.labels.shape[1]
num_features = mnist.train.images.shape[1]
training_data = mnist.train
testing_data = mnist.test

tf.reset_default_graph()

x_input = tf.placeholder(dtype=tf.float32, shape=[None, num_features], name='x_input')
y_input = tf.placeholder(dtype=tf.float32, shape=[None, num_classes], name='actual_label')

x_in = x_input
y_in = y_input

# Creating DoG filters

def dog_on(n,v,p):

  nr = np.floor(n/2) # half the size
  t = p * v # std deviation of surround (s2)
  
  x,y = np.mgrid[-nr:nr,-nr:nr]
  
  H  = 10*(1/(2 * np.pi * (v**2) )) * np.exp(-( (x**2) + (y**2) )/( 2 * (v**2) ))         # gaussian for center
  K  = 10*(1/(2 * np.pi * (t**2) )) * np.exp(-( (x**2) + (y**2) )/( 2 * (t**2) ))         # gaussian for surround
  
  s1 = np.sum( np.sum(H) )
  s2 = np.sum( np.sum(K) )
  
  D = ((s2 * H) - (s1 * K))
  return D

off_midget = -(dog_on(5,0.8,6.7))     # midget off center
on_midget = dog_on(11,1.04,6.7)    # midget on center
off_parasol = -(dog_on(61,8,4.8))     # parasol off center
on_parasol = dog_on(243,10.4,4.8)  # parasol on center


plt.figure()
plt.imshow( off_midget ,cmap="gray")
plt.axis('off')

plt.figure()
plt.imshow( on_midget ,cmap="gray")
plt.axis('off')

plt.figure()
plt.imshow( off_parasol ,cmap="gray")
plt.axis('off')

plt.figure()
plt.imshow( on_parasol ,cmap="gray")
plt.axis('off')


def DoG_conv(image):

  first_img = np.reshape( image, (28,28))
  on_conv = correlate(first_img, on_filt, mode='constant')
  off_conv = correlate( first_img, off_filt, mode='constant')

  final_img = np.zeros( (first_img.shape[0], first_img.shape[1]), dtype = np.float32 )
  for i in range(first_img.shape[0]):
    for j in range(first_img.shape[1]):
      final_img[i][j] = np.maximum( on_conv[i][j], off_conv[i][j] )
  
  return np.reshape(final_img,(784,))

# Filtering with all four DoG cells

sampling_size = 100
train_set = training_data.images[:sampling_size]
test_set = testing_data.images[:sampling_size]

# midget cells

moff_train = []
print(train_set.shape)
modulus = train_set.shape[0]
for i in train_set:
  img = np.reshape(i, (28,28) )
  img = correlate(img, off_midget, mode='constant')
  moff_train.append( np.reshape( img,(784,)) )
  
mon_train = []
print(train_set.shape)
modulus = train_set.shape[0]
for i in train_set:
  img = np.reshape(i, (28,28) )
  img = correlate(img, on_midget, mode='constant')
  mon_train.append( np.reshape( img,(784,)) )

moff_test = []
print(test_set.shape)
for i in test_set:
  img = np.reshape(i, (28,28) )
  img = correlate(img, off_midget, mode='constant')
  moff_test.append( np.reshape( img,(784,)) )
  
mon_test = []
print(test_set.shape)
for i in test_set:
  img = np.reshape(i, (28,28) )
  img = correlate(img, on_midget, mode='constant')
  mon_test.append( np.reshape( img,(784,)) )

# parasol cells

poff_train = []
print(train_set.shape)
modulus = train_set.shape[0]
for i in train_set:
  img = np.reshape(i, (28,28) )
  img = correlate(img, off_parasol, mode='constant')
  poff_train.append( np.reshape( img,(784,)) )
  
pon_train = []
print(train_set.shape)
modulus = train_set.shape[0]
for i in train_set:
  img = np.reshape(i, (28,28) )
  img = correlate(img, on_parasol, mode='constant')
  pon_train.append( np.reshape( img,(784,)) )

poff_test = []
print(test_set.shape)
for i in test_set:
  img = np.reshape(i, (28,28) )
  img = correlate(img, off_parasol, mode='constant')
  poff_test.append( np.reshape( img,(784,)) )
  
pon_test = []
print(test_set.shape)
for i in test_set:
  img = np.reshape(i, (28,28) )
  img = correlate(img, on_parasol, mode='constant')
  pon_test.append( np.reshape( img,(784,)) )

# Outputs with individual filters

for i in range(3):
    plt.figure()
    plt.imshow( np.reshape(moff_train[i], (28,28)),cmap="gray")
    plt.axis('off')
    plt.title(str(np.argmax(training_data.labels[i])));
    
for i in range(3):
    plt.figure()
    plt.imshow( np.reshape(mon_train[i], (28,28)),cmap="gray")
    plt.axis('off')
    plt.title(str(np.argmax(training_data.labels[i])));
    
for i in range(3):
    plt.figure()
    plt.imshow( np.reshape(poff_train[i], (28,28)),cmap="gray")
    plt.axis('off')
    plt.title(str(np.argmax(training_data.labels[i])));
    
for i in range(3):
    plt.figure()
    plt.imshow( np.reshape(pon_train[i], (28,28)),cmap="gray")
    plt.axis('off')
    plt.title(str(np.argmax(training_data.labels[i])));

final_train = []
for k in range(len(pon_train)):
  img = np.zeros((28,28))
  img1 = np.reshape(pon_train[k], (28,28))
  img2 = np.reshape(poff_train[k], (28,28))
  img3 = np.reshape(mon_train[k], (28,28))
  img4 = np.reshape(moff_train[k], (28,28))
  for i in range(28):
    for j in range(28):
      max_val = max( max(img1[i][j], img2[i][j]), max(img3[i][j], img4[i][j]) )
      img[i][j] = max_val
  final_train.append(img)

final_test = []
for k in range(len(pon_train)):
  img = np.zeros((28,28))
  img1 = np.reshape(pon_test[k], (28,28))
  img2 = np.reshape(poff_test[k], (28,28))
  img3 = np.reshape(mon_test[k], (28,28))
  img4 = np.reshape(moff_test[k], (28,28))
  for i in range(28):
    for j in range(28):
      max_val = max( max(img1[i][j], img2[i][j]), max(img3[i][j], img4[i][j]) )
      img[i][j] = max_val
  final_test.append(img)

for i in range(3):
    plt.figure()
    plt.imshow( np.reshape(final_train[i], (28,28)),cmap="gray")
    plt.axis('off')
    plt.title(str(np.argmax(training_data.labels[i])));

moff_train = []
modulus = training_data.images.shape[0]
for i in training_data.images:
  img = np.reshape(i, (28,28) )
  img = correlate(img, off_midget, mode='constant')
  moff_train.append( np.reshape( img,(784,)) )
  
moff_test = []
for i in testing_data.images:
  img = np.reshape(i, (28,28) )
  img = correlate(img, off_midget, mode='constant')
  moff_test.append( np.reshape( img,(784,)) )
  
train_features = np.asarray(moff_train, dtype=np.float32)
train_labels = np.asarray(training_data.labels, dtype=np.float32)
test_features = np.asarray(moff_test, dtype=np.float32)
test_labels = np.asarray(testing_data.labels, dtype=np.float32)

print(train_features.shape, test_features.shape)

# The SCNN network

import matplotlib.pyplot as plt

with nengo.Network() as net:
    net.config[nengo.Ensemble].max_rates = nengo.dists.Choice([100])
    net.config[nengo.Ensemble].intercepts = nengo.dists.Choice([0])
    neuron_type = nengo.LIF()

    nengo_dl.configure_settings(trainable=False)

    # the input node that will be used to feed in input images
    inp = nengo.Node([0] * 28 * 28)      # else 28 * 56

    out_1 = nengo.Probe(inp)

    # add the first convolutional layer
    x = nengo_dl.tensor_layer( inp, tf.layers.conv2d, shape_in=(28, 28, 1), filters=32, kernel_size=3)
    
    out_2 = nengo.Probe(x)
    
    # apply the neural nonlinearity
    x = nengo_dl.tensor_layer(x, neuron_type)

    # add another convolutional layer
    x = nengo_dl.tensor_layer(
        x, tf.layers.conv2d, shape_in=(26, 26, 32),
        filters=64, kernel_size=3)
    
    out_3 = nengo.Probe(x)
    
    x = nengo_dl.tensor_layer(x, neuron_type)

    # add a pooling layer
    x = nengo_dl.tensor_layer(
        x, tf.layers.average_pooling2d, shape_in=(24, 24, 64),
        pool_size=2, strides=2)
    
    out_4 = nengo.Probe(x)

    # another convolutional layer
    x = nengo_dl.tensor_layer(
        x, tf.layers.conv2d, shape_in=(12, 12, 64),
        filters=128, kernel_size=3)
    
    out_5 = nengo.Probe(x)
      
    x = nengo_dl.tensor_layer(x, neuron_type)

    # another pooling layer
    x = nengo_dl.tensor_layer(
        x, tf.layers.average_pooling2d, shape_in=(10, 10, 128),
        pool_size=2, strides=2)
    
    out_6 = nengo.Probe(x)

    x = nengo_dl.tensor_layer(x, tf.layers.dense, units=10)

    out_p = nengo.Probe(x)
    out_p_filt = nengo.Probe(x, synapse=0.1)

minibatch_size = 200
sim = nengo_dl.Simulator(net, minibatch_size = minibatch_size)

# add the single timestep to the training data
train_data = {inp: train_features[:, None, :], out_p: train_labels[:, None, :]}

n_steps = 30
test_data = {inp: np.tile(test_features[:minibatch_size*2, None, :], (1, n_steps, 1)), out_p_filt: np.tile(test_labels[:minibatch_size*2, None, :], (1, n_steps, 1))}

def objective(outputs, targets):
    return tf.nn.softmax_cross_entropy_with_logits_v2(logits=outputs, labels=targets)
  
opt = tf.train.AdamOptimizer(learning_rate=0.01)

def classification_accuracy(outputs,targets):
    print(outputs.shape, targets.shape)
    correct_prediction = tf.equal(tf.argmax(outputs[:, -1], axis=-1), tf.argmax(targets[:, -1], axis=-1))
    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32)) * 100.0
    return accuracy

def classification_error(outputs, targets):
    return 100 * tf.reduce_mean(tf.cast(tf.not_equal(tf.argmax(outputs[:, -1], axis=-1), tf.argmax(targets[:, -1], axis=-1)), tf.float32))

print("accuracy before training: %.2f%%" % sim.loss(test_data, {out_p_filt: classification_accuracy}))

do_training = True
if do_training:
    sim.train(train_data, opt, objective={out_p: objective}, n_epochs=3)

print("accuracy after training: %.2f%%" % sim.loss(test_data, {out_p_filt: classification_accuracy}))

import matplotlib.pyplot as plt
sim.run_steps(n_steps, data={inp: test_data[inp][:minibatch_size]})

# Visualizing the intermediate outputs

for s in range(4,6):         # for timestep
  print(" the timestep is:",s)
  for i in range(2):         # for input image number in the dataset
      print("the image is:")
      plt.figure()
      plt.imshow(np.reshape(test_data[inp][i,s], (28, 28)),cmap="gray")
      plt.axis('off')
      plt.show()

      print("outputs of input layer")

      plt.figure()
      plot0 = sim.data[out_1][i,s].reshape((28,28))
      plt.imshow(plot0[:,:].astype(np.uint8))
      plt.axis('off')
      plt.show()

      print("outputs of 1st conv layer")

      plt.figure()
      plot1 = sim.data[out_2][i,s].reshape((26,26,32))

      for k in range(16):
        plt.subplot(4, 4, k+1)
        plt.imshow(plot1[:,:,k].astype(np.uint8))
        plt.axis('off')

      plt.show()

      print("outputs of 2nd conv layer")

      plt.figure()
      plot2 = sim.data[out_3][i,s].reshape((24,24,64))

      for k in range(16):
        plt.subplot(4, 4, k+1)
        plt.imshow(plot2[:,:,k].astype(np.uint8))
        plt.axis('off')

      plt.show()

      print("outputs of 2nd max pool layer")

      plt.figure()
      plot3 = sim.data[out_4][i,s].reshape((12,12,64))

      for k in range(16):
        plt.subplot(4, 4, k+1)
        plt.imshow(plot3[:,:,k].astype(np.uint8))
        plt.axis('off')

      plt.show()

      print("outputs of 3rd conv layer")

      plt.figure()
      plot4 = sim.data[out_5][i,s].reshape((10,10,128))

      for k in range(16):
        plt.subplot(4, 4, k+1)
        plt.imshow(plot4[:,:,k].astype(np.uint8))
        plt.axis('off')

      plt.show()

      print("outputs of 3rd max pool layer")

      plt.figure()
      plot5 = sim.data[out_6][i,s].reshape((5,5,128))

      for k in range(16):
        plt.subplot(4, 4, k+1)
        plt.imshow(plot5[:,:,k].astype(np.uint8))
        plt.axis('off')

      plt.show()

for i in range(5):
    plt.figure()
    plt.subplot(1, 2, 1)
    plt.imshow(np.reshape(test_data[inp][i, 0], (28, 28)),
               cmap="gray")
    plt.axis('off')

    plt.subplot(1, 2, 2)
    plt.plot(sim.trange(), sim.data[out_p_filt][i])
    plt.legend([str(i) for i in range(10)], loc="upper left")
    plt.xlabel("time")

sim.close()
